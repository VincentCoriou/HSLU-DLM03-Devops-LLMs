{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b712c29f0c51d42",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e193ccd8-0282-4045-941c-3e2368097c37",
   "metadata": {},
   "source": [
    "## Introducion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a2450c-1911-4937-82a9-2e8c6b270f7a",
   "metadata": {},
   "source": [
    "The rapid growth of information and the complexity of modern queries have revealed a significant challenge for\n",
    "traditional Natural Language Processing (NLP) systems. While large language models (LLMs) like GPT-4 and Gemini excel at\n",
    "generating coherent and contextually relevant text, they often face a fundamental limitation: their knowledge is\n",
    "confined to the data they were trained on. This can lead to \"hallucinations\" (factually incorrect information) and an\n",
    "inability to answer questions about recent or private, domain-specific data.\n",
    "\n",
    "A Retrieval-Augmented Generation (RAG) pipeline addresses these limitations by combining the strengths of two\n",
    "approaches:\n",
    "\n",
    "1. A powerful **retriever** that pulls relevant, up-to-date, or proprietary information from an external knowledge base.\n",
    "2. A **generator** (an LLM) that synthesizes this retrieved information into a clear and natural-sounding response.\n",
    "\n",
    "By implementing a RAG pipeline, we can build a system that is more accurate, up-to-date, and trustworthy.\n",
    "\n",
    "### A Real-World Use Case: RAG for Software Engineering\n",
    "\n",
    "Imagine you're a new developer at a large tech company. The company has thousands of microservices, extensive internal\n",
    "libraries, and decades of documentation scattered across different platforms. Finding the right information, how to use a\n",
    "specific API, the purpose of a legacy service, or the solution to a cryptic errorâ€”can be a significant bottleneck.\n",
    "\n",
    "This is where a RAG pipeline becomes a superpower for developers. Instead of manually searching through wikis, code\n",
    "repositories, and old design documents, you could ask a specialized assistant:\n",
    "\n",
    "- *\"What's the correct way to implement authentication for the 'Triton' service?\"*\n",
    "- *\"Show me an example of how to use the `Unicorn` library's data processing module in Python.\"*\n",
    "- *\"What was the reasoning behind the last major change to the 'Phoenix' microservice?\"*\n",
    "\n",
    "A RAG system connected to the company's internal knowledge base (documentation, code, architectural records) can\n",
    "retrieve the most relevant information and use it to generate a direct, accurate, and code-supported answer. This boosts\n",
    "developer productivity, improves knowledge sharing, and accelerates onboarding. In this project, you will build the core\n",
    "components of such a system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdc1bb9-4372-43e1-a801-edfcbb2f14b9",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de73ac10c72d9b0c",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6953ed2c73a5eb",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9191ac20d9f7a7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import initialize_notebook\n",
    "\n",
    "import abc\n",
    "import dataclasses\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "from collections.abc import Sequence\n",
    "from typing import Literal, Protocol, override\n",
    "\n",
    "import jinja2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import openai\n",
    "import pydantic\n",
    "import tqdm\n",
    "from hslu.dlm03.rag import metrics\n",
    "from hslu.dlm03.rag import util\n",
    "from hslu.dlm03.util import ratelimit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830e2330-f7b8-4b0b-b429-9131c9a7537e",
   "metadata": {},
   "source": [
    "### Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8555d0-1064-4382-a4d8-dbf8eb38d4b6",
   "metadata": {},
   "source": [
    "First, let's define the basic data structures that will be used for the pipeline.\n",
    "\n",
    "1. **Query**: this data structure will hold the user's query text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f7aa48-2cd1-45f4-abfd-f9affc2599b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass()\n",
    "class Query:\n",
    "    text: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af78a649-1edb-4f53-9059-f6a051f2b59d",
   "metadata": {},
   "source": [
    "2. **Answer**: this data structure will hold the models/pipeline answer text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f2d6d4-4c3c-4c24-8ea8-51ed103d256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass()\n",
    "class Answer:\n",
    "    text: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f96db25-656d-4163-9800-42c980f7cfb8",
   "metadata": {},
   "source": [
    "3. **Document**: this data structure will hold a document's content with its corresponding metadata and identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca8cf68-180f-4751-ba53-0af8ea991d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass()\n",
    "class Document:\n",
    "    id: str\n",
    "    text: str\n",
    "    metadata: dict[str, str]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46655f47-fb71-4f95-a6e4-0c06d9c89f76",
   "metadata": {},
   "source": [
    "4. **Corpus**: this data structure will hold a set of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebdde65-88f6-4143-b3a2-a2e8da8f9131",
   "metadata": {},
   "outputs": [],
   "source": [
    "Corpus = Sequence[Document]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5ab49aabbc6e46",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eb0b45-d9f6-4e74-a362-34b853f4d9d7",
   "metadata": {},
   "source": [
    "For a RAG pipeline, we require a set of `Document` to help the model retrieve information from.\n",
    "\n",
    "Additionally we require a dataset of `Query` to pass to the model, with their additional expected\n",
    "`Answer` for evaluation.\n",
    "\n",
    "Optionally, we can also associate a list of relevant `Document` that contain the information to\n",
    "properly answer the `Query`, which allows us to also measure the performance of internal pipeline components\n",
    "independently; thus we propose to define data samples as triplets of `(Query, Answer, Sequence[Document])` (and with additionally some metadata)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48a7cd3-be82-4514-932f-6867c5b73272",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass()\n",
    "class QueryAnswerDocument:\n",
    "    query: Query\n",
    "    answer: Answer\n",
    "    relevant_documents: Sequence[Document]\n",
    "    metadata: dict[str, str]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233c705f-0ff5-4161-9558-40d5d02c6f33",
   "metadata": {},
   "source": [
    "We will also define a sequence of these data samples as a `QuestionAnswerDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e89afa1-9001-4b2a-aef1-30abd88a1a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "QuestionAnswerDataset = Sequence[QueryAnswerDocument]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0f3422-289a-455c-b337-880aebc75290",
   "metadata": {},
   "source": [
    "We will be using Kaggle Question-Answer dataset for this work, but feel free to load any other dataset to experiment with.\n",
    "\n",
    "The dataset is available [here](https://www.kaggle.com/datasets/rtatman/questionanswer-dataset?resource=download)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe35477a-0d24-4a80-99fc-fb760a563f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KaggleQADataset:\n",
    "    @classmethod\n",
    "    def load_qa(cls, row: pd.Series, source: pathlib.Path, document_index: dict[str, int]) -> QueryAnswerDocument:\n",
    "        relevant_document = row[\"ArticleFile\"]\n",
    "        relevant_documents = [document_index[relevant_document]] if isinstance(relevant_document, str) else []\n",
    "        return QueryAnswerDocument(\n",
    "            query=Query(text=row[\"Question\"]),\n",
    "            answer=Answer(text=row[\"Answer\"]),\n",
    "            relevant_documents=relevant_documents,\n",
    "            metadata={\n",
    "                \"source\": source,\n",
    "                \"question_difficulty\": row[\"DifficultyFromQuestioner\"],\n",
    "                \"answer_difficulty\": row[\"DifficultyFromAnswerer\"],\n",
    "            },\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def read_document(cls, path: pathlib.Path) -> Document:\n",
    "        filename = path.name\n",
    "        content = path.read_text(encoding=\"latin-1\")\n",
    "        title, content = content.split(\"\\n\", 1)\n",
    "        document_id = filename.split(\".\", 1)[0]\n",
    "        return Document(\n",
    "            id=document_id,\n",
    "            text=content.strip(),\n",
    "            metadata={\"source\": filename, \"title\": title},\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def load(\n",
    "            cls,\n",
    "            path: pathlib.Path,\n",
    "            sets: str | Sequence[str] = (\"S08\", \"S09\", \"S10\"),\n",
    "            sample: int | None = None,\n",
    "    ) -> tuple[QuestionAnswerDataset, Corpus]:\n",
    "        if isinstance(sets, str):\n",
    "            sets = (sets,)\n",
    "        documents_path = path / \"text_data\"\n",
    "        documents = []\n",
    "        for filename in documents_path.iterdir():\n",
    "            if any(s in str(filename) for s in sets):\n",
    "                document_path = documents_path / filename\n",
    "                document = cls.read_document(document_path)\n",
    "                documents.append(document)\n",
    "        document_index = {document.id: document for document in documents}\n",
    "        qa_dataset = []\n",
    "        for filename in path.iterdir():\n",
    "            if any(s in str(filename) for s in sets):\n",
    "                dataset = pd.read_csv(\n",
    "                    path / filename, sep=\"\\t\", encoding=\"latin-1\",\n",
    "                )\n",
    "                dataset = dataset.dropna(axis=0)\n",
    "                for _, row in dataset.iterrows():\n",
    "                    qa = cls.load_qa(row, filename, document_index)\n",
    "                    qa_dataset.append(qa)\n",
    "\n",
    "        if sample is not None:\n",
    "            random.shuffle(qa_dataset)\n",
    "            qa_dataset = qa_dataset[:sample]\n",
    "        return qa_dataset, documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05cd6ff-9ba0-434f-9a7f-2d42b70fa37a",
   "metadata": {},
   "source": [
    "We will load the entirety of the dataset, as well as a partial dataset to allow for faster evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2417a90a292c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = pathlib.Path(os.environ.get(\"DATA_PATH\"))\n",
    "dataset_path = DATA_PATH / \"datasets\" / \"questionanswer-dataset\"\n",
    "\n",
    "qa_dataset, documents = KaggleQADataset.load(dataset_path, sample=None, sets=(\"S08\", \"S09\", \"S10\"))\n",
    "partial_dataset, _ = KaggleQADataset.load(dataset_path, sample=50, sets=(\"S08\", \"S09\", \"S10\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7f672a-0dcb-44fb-8510-90a72daaaa37",
   "metadata": {},
   "source": [
    "Below is an example of a sample from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5216edaf-0289-44f8-95c7-fdaf9c725e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = qa_dataset[35] # 35\n",
    "print(\"Query:\", sample.query.text)\n",
    "print(\"Answer:\", sample.answer.text)\n",
    "for i, document in enumerate(sample.relevant_documents):\n",
    "    print(f\"Document {i} ({document.id}):\", document.text) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694001f9-6993-4460-b5f4-dbc529870f0d",
   "metadata": {},
   "source": [
    "Using this dataset, we can already highlight the weakness of LLMs for factual query answering, by probing an LLM with one of the queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d378d6-c059-4a63-b2f9-aaedf372045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass(frozen=True)\n",
    "class LLM:\n",
    "    system_prompt: str\n",
    "    client: openai.Client\n",
    "    model_name: str\n",
    "    ratelimiter: ratelimit.RateLimiter\n",
    "    \n",
    "    def answer(self, query: Query) -> Answer:\n",
    "        with self.ratelimiter:\n",
    "            response = self.client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": query.text},\n",
    "                ],\n",
    "                model=self.model_name,\n",
    "            )\n",
    "            choice = random.choice(response.choices)\n",
    "            return Answer(text=choice.message.content)\n",
    "\n",
    "    def __call__(self, query: Query) -> Answer:\n",
    "        return self.answer(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc75746c-a10a-4afc-812a-e0b248062633",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_BASE_URL = \"http://localhost:8080/v1\"\n",
    "MODEL_NAME = \"...\"\n",
    "MODEL_API_KEY = \"...\"\n",
    "MODEL_RPM = 1e6\n",
    "\n",
    "MODEL_SYSTEM_PROMPT = \"\"\"Please answer the given user query consicely (do not write full sentences, just provide a direct answer).\"\"\"\n",
    "\n",
    "model_client = openai.Client(base_url=MODEL_BASE_URL, api_key=MODEL_API_KEY)\n",
    "model = LLM(client=model_client, model_name=MODEL_NAME, ratelimiter=ratelimit.RateLimiter(rpm=MODEL_RPM), system_prompt=MODEL_SYSTEM_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eba5f30-0da2-4457-8fed-061a0a8f4c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = model.answer(sample.query)\n",
    "print(\"Query:\", sample.query.text)\n",
    "print(\"---------------\")\n",
    "print(\"Generated:\", answer.text)\n",
    "print(\"---------------\")\n",
    "print(\"Expected:\", sample.answer.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7358687-5df6-4255-a96a-cd3fa7088592",
   "metadata": {},
   "source": [
    "### Embedder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bcd38b-2df9-4f20-aa1d-5bb79fe958c6",
   "metadata": {},
   "source": [
    "We refer as `Embedding` a numerical vector whose goal is to hold a semantic representation of a given text.\n",
    "We will use `numpy` to store this numerical representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13ed327-8b38-4daf-bdb4-6422df17adb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Embedding = np.ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b80e5b9-911d-46a4-b579-0f6a938aa556",
   "metadata": {},
   "source": [
    "We will define an `Embedder` as an interface that can transform a text into an embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9986960b-4542-4aec-8eaf-86fa253e3bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(Protocol):\n",
    "    def embed(self, text: str) -> Embedding:\n",
    "        ...\n",
    "\n",
    "    def embed_query(self, query: Query) -> Embedding:\n",
    "        return self.embed(query.text)\n",
    "\n",
    "    def embed_document(self, document: Document) -> Embedding:\n",
    "        return self.embed(document.text)\n",
    "\n",
    "    def embed_documents(\n",
    "            self, documents: Sequence[Document], *, progress: bool = False,\n",
    "    ) -> Sequence[Embedding]:\n",
    "        if progress:\n",
    "            documents = tqdm.tqdm(documents)\n",
    "        return [self.embed_document(document) for document in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830d32e2-93b8-46b5-be23-c3b77ffceb40",
   "metadata": {},
   "source": [
    "As a first approach, we propose to use LLM-based embeddings methods using the [OpenAI Embedding API](https://platform.openai.com/docs/guides/embeddings), by\n",
    "defining an `LLMEmbedder`.\n",
    "\n",
    "**Note**: Some Embeddings server cannot handle more than a certain context window, so you might need to truncate your texts before sending them to the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e284ae-b3d1-45fe-837a-79344f53835b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass(frozen=True)\n",
    "class LLMEmbedder(Embedder):\n",
    "    client: openai.Client\n",
    "    model_name: str\n",
    "    ratelimiter: ratelimit.RateLimiter\n",
    "\n",
    "    def embed(self, text: str) -> Embedding:\n",
    "        with self.ratelimiter:\n",
    "            response = self.client.embeddings.create(\n",
    "                input=text[:5000], model=self.model_name,\n",
    "            )\n",
    "            return np.array(response.data[0].embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221e20ffb5d67a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_BASE_URL = \"http://localhost:8081/v1\"\n",
    "EMBEDDING_MODEL_NAME = \"...\"\n",
    "EMBEDDING_API_KEY = \"...\"\n",
    "EMBEDDING_RPM = 1e6\n",
    "\n",
    "embedding_client = openai.Client(base_url=EMBEDDING_BASE_URL, api_key=EMBEDDING_API_KEY)\n",
    "embedder = LLMEmbedder(client=embedding_client, model_name=EMBEDDING_MODEL_NAME, ratelimiter=ratelimit.RateLimiter(rpm=EMBEDDING_RPM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1721065-4d7a-44ed-983b-d9833c6f37b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedder.embed_query(sample.query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a35c4d0-85c6-422d-9357-fbc423b18463",
   "metadata": {},
   "source": [
    "### Embedding Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0c3596-7463-4810-b53b-85503821d168",
   "metadata": {},
   "source": [
    "`Embeddings` can be used to create structured spaces, in which we can define distance/similarity metrics to efficiently navigate the space.\n",
    "\n",
    "We will define `Similarity` between `Embedding` using `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489c67f7-0875-4a7f-8aee-1156b3babd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "Similarity = np.ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f397e0-1089-4995-bf1e-0112dde5146e",
   "metadata": {},
   "source": [
    "We will refer to as a similarity function any function that computes the similarity between 2 `Embedding` and returns a `Similarity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f85297f-559a-4410-bda6-3e34b9157db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilarityFn(Protocol):\n",
    "    def __call__(self, left: Embedding, right: Embedding, /,) -> Similarity:\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93817db1-8886-4264-8913-9cf874c1f170",
   "metadata": {},
   "source": [
    "**Note**: Please ensure that the similarity function you used for the Vector Store is the same as the one the model\n",
    "was trained for, or you risk ending up with irrelevant documents being retrieved. Please refer to the model's official\n",
    "documentation for closed source model or the model's card on Hugging Face for open weights models.\n",
    "\n",
    "The most commonly used similarity functions are:\n",
    "\n",
    "- **Cosine Similarity**: $$\\text{sim}(x, y) = \\frac{x \\cdot y}{|x|\\times|y|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd05045c-bf63-40c7-a024-1b72c4d560c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(left: Embedding, right: Embedding, /) -> Similarity:\n",
    "    if left.shape[-1] != right.shape[-1]:\n",
    "        error_message = f\"Expected embeddings to have same dimension, but got {left.shape[-1]} and {right.shape[-1]}\"\n",
    "        raise ValueError(error_message)\n",
    "    left, right = util.expand_match_dims(left, right, sizes=(left.ndim - 1, right.ndim - 1))\n",
    "    left_norm = np.linalg.norm(left, axis=-1)\n",
    "    right_norm = np.linalg.norm(right, axis=-1)\n",
    "    left, right = np.broadcast_arrays(left, right)\n",
    "    dot_product = np.matmul(left[..., None, :], right[..., None]).squeeze((-1, -2))\n",
    "    return dot_product / (left_norm * right_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c62195-44b9-4667-9c5b-6e9285d3d53b",
   "metadata": {},
   "source": [
    "- **Euclidean Similarity**: $$\\text{sim}(x, y) = \\frac{1}{1 + d(x, y)},\\qquad d(x, y) = \\sqrt{\\sum (x_i - y_i)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c152c949-97c5-426b-bea1-df83b2d949ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_similarity(left: Embedding, right: Embedding, /) -> Similarity:\n",
    "    if left.shape[-1] != right.shape[-1]:\n",
    "        error_message = f\"Expected embeddings to have same dimension, but got {left.shape[-1]} and {right.shape[-1]}\"\n",
    "        raise ValueError(error_message)\n",
    "    left, right = util.expand_match_broadcast(left, v, sizes=(left.ndim - 1, v.ndim - 1))\n",
    "    euclidean_distance = np.sqrt(np.power(left - right, 2).sum(-1))\n",
    "    return 1 / (1 + euclidean_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d588a969-c25f-4535-a8e1-421e7acde7e3",
   "metadata": {},
   "source": [
    "### Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6fd385-add8-4fdb-8c7a-4b6273d79b24",
   "metadata": {},
   "source": [
    "The retriever's job is to find the most relevant documents from a given corpus that contain information that are\n",
    "relevant to answer a given query.\n",
    "\n",
    "We will define a `Retriever` as an interface that given a `Query` returns an (ordered) sequence of `Document`.\n",
    "The `Retriever` should hold the corpus of `Document` to retrieve from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bdfb0b-500b-4bfb-957b-7fa650b6c124",
   "metadata": {},
   "outputs": [],
   "source": [
    "DocumentIndex = dict[str, int]\n",
    "\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class Retriever(abc.ABC):\n",
    "    documents: Corpus\n",
    "    document_index: DocumentIndex\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def retrieve(self, text: Query, k: int | None = None) -> tuple[Sequence[Document], Similarity]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __call__(self, text: Query, k: int | None = None) -> Sequence[Document]:\n",
    "        documents, _ = self.retrieve(text, k)\n",
    "        return documents\n",
    "    \n",
    "    @staticmethod\n",
    "    def index(documents: Corpus) -> DocumentIndex:\n",
    "        document_index = {}\n",
    "        for i, document in enumerate(documents):\n",
    "            document_index[document.id] = i\n",
    "        return document_index\n",
    "\n",
    "    @classmethod\n",
    "    def from_documents(cls, documents: Corpus, **kwargs) -> 'Retriever':\n",
    "        document_index = cls.index(documents)\n",
    "        return cls(documents=documents, document_index=document_index, **kwargs)\n",
    "\n",
    "    def get_document_by_index(self, index: int) -> Document:\n",
    "        return self.documents[index]\n",
    "\n",
    "    def get_index_by_id(self, document_id: str) -> int:\n",
    "        return self.document_index[document_id]\n",
    "\n",
    "    def get_document_by_id(self, document_id: str) -> Document:\n",
    "        index = self.get_index_by_id(document_id)\n",
    "        return self.get_document_by_index(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925d80c2-b17c-44a2-8ac0-ea79766bdfe8",
   "metadata": {},
   "source": [
    "A special case of `Retriever`, based on using `Embedding` to represent `Query` and `Document` and compute the `Similarity` between them is referred to as\n",
    "a [Vector Store](https://en.wikipedia.org/wiki/Vector_database). A `VectorStore` is composed of 3 main components:\n",
    "\n",
    "- A `Corpus` of documents to retrieve from (part of the `Retriever`).\n",
    "- An `Embedder` to use to embed both documents and queries into a common vector space.\n",
    "- A `SimilarityFn` used to compute the similarity between query embeddings and document embeddings.\n",
    "\n",
    "A Vector Store works by precomputing the embeddings of all the documents in the corpus, and when probed with a query, it\n",
    "embeds it and computes the similarity between the query and each of the documents in the corpus and returns the top-$k$\n",
    "most similar documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bb09c6-4f15-4d2d-989c-7c060348d315",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass(frozen=True)\n",
    "class VectorStore(Retriever):\n",
    "    embedder: Embedder\n",
    "    similarity_fn: SimilarityFn\n",
    "    embeddings: Embedding\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def embeddings_matrix(\n",
    "            documents: Corpus,\n",
    "            embedder: Embedder,\n",
    "            *,\n",
    "            progress: bool = True,\n",
    "    ) -> Embedding:\n",
    "        document_embeddings = embedder.embed_documents(documents, progress=progress)\n",
    "        return np.stack(document_embeddings, axis=0)\n",
    "\n",
    "    def retrieve(\n",
    "            self, query: Query, k: int | None = None,\n",
    "    ) -> tuple[np.ndarray, Similarity]:\n",
    "        if k is None:\n",
    "            k = len(self.documents)\n",
    "        embedding = self.embedder.embed(query.text)\n",
    "        similarities = self.similarity_fn(embedding, self.embeddings)\n",
    "        top_k_indices = np.argsort(-similarities)[..., :k]\n",
    "        similarities = np.take_along_axis(similarities, top_k_indices, axis=-1)\n",
    "        documents = np.array(\n",
    "            [self.get_document_by_index(i) for i in np.ravel(top_k_indices)],\n",
    "        ).reshape(top_k_indices.shape)\n",
    "        return documents, similarities\n",
    "\n",
    "    @classmethod\n",
    "    def from_documents(cls, documents: Corpus, *, embedder: Embedder, **kwargs):\n",
    "        return super().from_documents(documents=documents, embedder=embedder, embeddings=cls.embeddings_matrix(documents, embedder), **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d8bfb16c19f870",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = VectorStore.from_documents(documents, embedder=embedder, similarity_fn=cosine_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6f9edf-ed6c-4daa-b2c6-420c272b3996",
   "metadata": {},
   "source": [
    "We have succesfully completed the implementation of the first component of the RAG pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac2cc94-bc3e-4ca6-8a91-194c21dde9a2",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "We can already start by evaluating the quality of our `Retriever` component (from which the overall quality of the RAG pipeline will depend). It can be evaluated using standard retrieval metrics:\n",
    "\n",
    "- **recall_at_k**: computes the ratio of relevant documents that are present in the top-$k$ retrieved documents.\n",
    "- **precision_at_k**: computes the ratio of documents that are relevant in to top-$k$ retrieved documents.\n",
    "- **mean_rank**: computes the average rank of the relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7a01df-6555-40fd-8e13-0d355254df21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_retrieval(\n",
    "        dataset: QuestionAnswerDataset,\n",
    "        retriever: Retriever,\n",
    "        *,\n",
    "        progress: bool = True,\n",
    ") -> list[list[str]]:\n",
    "    if progress:\n",
    "        dataset = tqdm.tqdm(dataset)\n",
    "    return [[document.id for document in retriever(sample.query)] for sample in dataset]\n",
    "\n",
    "def evaluate_retrieval(\n",
    "    dataset: QuestionAnswerDataset,\n",
    "    retriever: Retriever,\n",
    "    k: int | Sequence[int],\n",
    "    *,\n",
    "    progress: bool = True,\n",
    ") -> dict[str, float]:\n",
    "    k = np.array(k)\n",
    "    rankings = infer_retrieval(dataset, retriever, progress=progress)\n",
    "    target_documents = [\n",
    "        [document.id for document in sample.relevant_documents] for sample in dataset\n",
    "    ]\n",
    "    rankings = np.array(rankings)\n",
    "    target_size = max(len(indices) for indices in target_documents)\n",
    "    targets = np.empty((len(target_documents), target_size), dtype=np.object_)\n",
    "    masks = np.zeros((len(target_documents), target_size), dtype=float)\n",
    "    for i, document_ids in enumerate(target_documents):\n",
    "        targets[i, : len(document_ids)] = document_ids\n",
    "        masks[i, : len(document_ids)] = 1\n",
    "    target_ranks = metrics.rank(targets, rankings)\n",
    "    results = {}\n",
    "\n",
    "    recalls = metrics.recall_at_k(target_ranks=target_ranks, k=k, mask=masks)\n",
    "    recalls = recalls[masks.sum(-1) > 0].mean(0)\n",
    "\n",
    "    precisions = metrics.precision_at_k(target_ranks=target_ranks, k=k, mask=masks)\n",
    "    precisions = precisions[masks.sum(-1) > 0].mean(0)\n",
    "\n",
    "    mean_ranks = metrics.mean_rank(target_ranks=target_ranks, mask=masks)\n",
    "    mean_ranks = mean_ranks[masks.sum(-1) > 0].mean(0)\n",
    "    for k_value, precision_at_k in zip(k, precisions, strict=False):\n",
    "        results[f\"precision@{k_value}\"] = precision_at_k.item()\n",
    "    for k_value, recall_at_k in zip(k, recalls, strict=False):\n",
    "        results[f\"recall@{k_value}\"] = recall_at_k.item()\n",
    "\n",
    "    results[\"mean_rank\"] = mean_ranks.item()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e25d69-204a-4e79-92e9-1b38aa6b3b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = np.array([1, 5, 10, 100])\n",
    "retrieval_results = evaluate_retrieval(qa_dataset, retriever, k=k)\n",
    "retrieval_results = pd.Series(retrieval_results)\n",
    "retrieval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3991604-d29d-4a1e-85c8-bee39687e1ed",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe57d3d-dc4f-4dc5-92b2-282bfb25b63a",
   "metadata": {},
   "source": [
    "The second component of the RAG pipeline is the `Generator`.\n",
    "It takes the user's query and the retrieved documents and produces a final, human-readable answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cf963e-e18a-4a9c-8060-27f5e1b432fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(Protocol):\n",
    "    def __call__(self, query: Query, documents: Sequence[Document]) -> Answer:\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe42af2f-4807-412f-848b-ff83bd2d5631",
   "metadata": {},
   "source": [
    "LLMs are well suited for generating text, and can be used to synthesize information from\n",
    "retrieved documents into a coherent answer. We will define an `LLMGenerator` class, which leverages the OpenAI API for\n",
    "text generation. The generation will be parametrized using a prompt template, using the `jinja2` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff1f9cc-be9b-4dd2-acd5-6b19f58d4336",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass(frozen=True)\n",
    "class LLMGenerator(Generator):\n",
    "    client: openai.Client\n",
    "    model_name: str\n",
    "    prompt_template: jinja2.Template\n",
    "    ratelimiter: ratelimit.RateLimiter\n",
    "\n",
    "    def prompt(self, query: Query, documents: Sequence[Document]) -> str:\n",
    "        return self.prompt_template.render(query=query, documents=documents)\n",
    "\n",
    "    def generate(self, query: Query, documents: Sequence[Document]) -> tuple[str, Answer]:\n",
    "        prompt = self.prompt(query, documents)\n",
    "        response = self.client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=self.model_name,\n",
    "        )\n",
    "        choice = random.choice(response.choices)\n",
    "        text = choice.message.content\n",
    "        return prompt, Answer(text=text)\n",
    "\n",
    "    def __call__(self, query: Query, documents: Sequence[Document]) -> Answer:\n",
    "        _, answer = self.generate(query, documents)\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54aad52c9fe74730",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATOR_BASE_URL = \"http://localhost:8080/v1\"\n",
    "GENERATOR_MODEL_NAME = \"...\"\n",
    "GENERATOR_API_KEY = \"...\"\n",
    "GENERATOR_RPM = 1e6\n",
    "\n",
    "GENERATOR_PROMPT_TEMPLATE = jinja2.Template(\"\"\"\n",
    "Given the following documents please answer the query below. Please give a concise and short answer in a few words. If non of the documents provide the answer, please output \"Unknown\".\n",
    "\n",
    "{% for document in documents %}\n",
    "Document {{ loop.index }}:\n",
    "{{ document.text }}\n",
    "{% endfor %}\n",
    "\n",
    "Query: {{ query.text }}\n",
    "\"\"\", undefined=jinja2.StrictUndefined)\n",
    "\n",
    "generator_client = openai.Client(base_url=GENERATOR_BASE_URL, api_key=GENERATOR_API_KEY)\n",
    "generator = LLMGenerator(prompt_template=GENERATOR_PROMPT_TEMPLATE, client=generator_client,\n",
    "                                    model_name=GENERATOR_MODEL_NAME, ratelimiter=ratelimit.RateLimiter(rpm=GENERATOR_RPM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75e4799-62d0-48f5-a130-77e255e051ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt, answer = generator.generate(sample.query, sample.relevant_documents)\n",
    "print(prompt)\n",
    "print(\"---------------\")\n",
    "print(\"Generated:\", answer.text)\n",
    "print(\"---------------\")\n",
    "print(\"Expected:\", sample.answer.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda0fafd-9663-4e57-a1db-f177b39bc09b",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "The text generation is inherently harder to evaluate, due to its free-from nature, but LLMs are also a good choice to compare 2 free text answers and return some signal of how much they match!\n",
    "\n",
    "We will use an LLM-based `Rater` to judge the quality of the generated answers compared to the dataset's reference answer.\n",
    "\n",
    "You can read more on this approach, called \"LLM-as-a-Judge\", [here](https://www.evidentlyai.com/llm-guide/llm-as-a-judge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfc6935-1a08-4021-8718-921f4ea517fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rate(Protocol):\n",
    "    def to_float(self) -> float:\n",
    "        ...\n",
    "\n",
    "class Rater[T: Rate](Protocol):\n",
    "    def rate(self, answer: Answer, expected: Answer) -> Rate:\n",
    "        ...\n",
    "\n",
    "    def __call__(self, answer: Answer, expected: Answer) -> float:\n",
    "        return self.rate(answer, expected).to_float()\n",
    "\n",
    "\n",
    "\n",
    "class Score(pydantic.BaseModel):\n",
    "    \"\"\"Class used to represent standard letter-scale scores.\"\"\"\n",
    "    score: Literal[\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]\n",
    "\n",
    "    def to_float(self) -> float:\n",
    "        \"\"\"Converts the score to a float value.\"\"\"\n",
    "        match self.score:\n",
    "            case \"A\":\n",
    "                return 1.0\n",
    "            case \"B\":\n",
    "                return 0.8\n",
    "            case \"C\":\n",
    "                return 0.6\n",
    "            case \"D\":\n",
    "                return 0.4\n",
    "            case \"E\":\n",
    "                return 0.2\n",
    "            case \"F\":\n",
    "                return 0.0\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class LLMRater[T: Rate](Rater):\n",
    "    prompt_template: jinja2.Template\n",
    "    client: openai.Client\n",
    "    model_name: str\n",
    "    response_format: type[T] | openai.NotGiven\n",
    "    ratelimiter: ratelimit.RateLimiter\n",
    "\n",
    "    def rate(self, answer: Answer, expected: Answer) -> T:\n",
    "        prompt = self.prompt_template.render(\n",
    "            answer=answer, expected=expected,\n",
    "        )\n",
    "        response = self.client.chat.completions.parse(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=self.model_name,\n",
    "            response_format=self.response_format,\n",
    "        )\n",
    "        choice = random.choice(response.choices)\n",
    "        return choice.message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54086e2c-2bb0-407f-a7b8-f819d2b68c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "RATER_BASE_URL = \"http://localhost:8080/v1\"\n",
    "RATER_MODEL_NAME = \"...\"\n",
    "RATER_API_KEY = \"...\"\n",
    "RATER_RPM = 1e6\n",
    "\n",
    "RATER_FORMAT = Score\n",
    "RATER_PROMPT_TEMPLATE = jinja2.Template(\"\"\"\n",
    "You will be provided with 2 answers, the first being the ground truth expected answer, and the second one being the actual generated answer, and you should rate the correctness of the generated answer from A to F (A being the best grade, F the worst).\n",
    "\n",
    "[Expected Answer]\n",
    "{{ expected.text }}\n",
    "\n",
    "[Actual Answer]\n",
    "{{ answer.text }}\n",
    "\n",
    "[Score]\n",
    "\"\"\", undefined=jinja2.StrictUndefined)\n",
    "\n",
    "rater_client = openai.Client(base_url=RATER_BASE_URL, api_key=RATER_API_KEY)\n",
    "rater = LLMRater(prompt_template=RATER_PROMPT_TEMPLATE, client=rater_client, model_name=RATER_MODEL_NAME,\n",
    "                        ratelimiter=ratelimit.RateLimiter(rpm=RATER_RPM), response_format=RATER_FORMAT)\n",
    "raters = {\"accuracy\": rater}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6059a624-e028-4afd-8b27-e3844cb2b688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_generator(\n",
    "        dataset: QuestionAnswerDataset,\n",
    "        generator: Generator,\n",
    "        *,\n",
    "        progress: bool = True,\n",
    ") -> list[Answer]:\n",
    "    if progress:\n",
    "        dataset = tqdm.tqdm(dataset)\n",
    "    return [\n",
    "        generator(sample.query, sample.relevant_documents)\n",
    "        for sample in dataset\n",
    "    ]\n",
    "\n",
    "def evaluate_answers(\n",
    "    actual_answers: list[Answer],\n",
    "    expected_answers: list[Answer],\n",
    "    raters: dict[str, Rater],\n",
    "    *,\n",
    "    progress: bool = True,\n",
    ") -> dict[str, float]:\n",
    "    results = {}\n",
    "    raters = raters.items()\n",
    "    if progress:\n",
    "        raters = tqdm.tqdm(raters, position=0)\n",
    "    for rater_name, rater in raters:\n",
    "        rater_values = []\n",
    "        data = zip(actual_answers, expected_answers, strict=False)\n",
    "        if progress:\n",
    "            data = tqdm.tqdm(data, position=0, total=len(actual_answers))\n",
    "        for actual_answer, expected_answer in data:\n",
    "            value = rater(actual_answer, expected_answer)\n",
    "            rater_values.append(value)\n",
    "        results[rater_name] = sum(rater_values) / len(rater_values)\n",
    "    return results\n",
    "\n",
    "def evaluate_generator(\n",
    "    dataset: QuestionAnswerDataset,\n",
    "    generator: Generator,\n",
    "    raters: dict[str, Rater],\n",
    "        *,\n",
    "    progress: bool = True,\n",
    ") -> tuple[dict[str, float], list[Answer]]:\n",
    "    answers = infer_generator(dataset, generator, progress=progress)\n",
    "    expected_answers = [sample.answer for sample in dataset]\n",
    "    results = evaluate_answers(answers, expected_answers, raters, progress=progress)\n",
    "    return results, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f80bc8-7e07-4052-adc5-dac4c8a69446",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_results, generator_answers = evaluate_generator(partial_dataset, generator, raters)\n",
    "generator_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf264b7e-8e2c-4413-be51-889be8a3888c",
   "metadata": {},
   "source": [
    "### RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4b60b6-7e23-4921-beb1-7c7e213b84f8",
   "metadata": {},
   "source": [
    "Now that we have implemented all the components, we can implement the high level RAG wrapper pipeline.\n",
    "\n",
    "The `RAG` pipeline should be parametrized from a `Retriever`, a `Generator` and a value `k` of documents to retrieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb746c7f-ec27-4e10-9ef4-6b1a91b366b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass(frozen=True)\n",
    "class RAG:\n",
    "    retriever: Retriever\n",
    "    generator: Generator\n",
    "    k: int\n",
    "\n",
    "    def generate(self, query: Query) -> tuple[Sequence[Document], Answer]:\n",
    "        documents = self.retriever(query, self.k)\n",
    "        answer = self.generator(query, documents)\n",
    "        return documents, answer\n",
    "\n",
    "    def __call__(self, query: Query) -> Answer:\n",
    "        _, answer = self.generate(query)\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285ceb879a3d2600",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 1\n",
    "rag = RAG(retriever=retriever, generator=generator, k=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79be29c2-be6c-45b7-9fa8-0e79b5da8ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, answer = rag.generate(sample.query)\n",
    "print(\"Query:\", sample.query)\n",
    "print(\"---------------\")\n",
    "print(\"Generated:\", answer.text)\n",
    "print(\"---------------\")\n",
    "print(\"Expected:\", sample.answer.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9cbcfb5ebd91a",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "The full RAG pipeline can be evaluated using the same metrics as the generator, but this time the documents passed to the prompt will be provided by the retriever (instead of being taken from the ground truth documents). We can also evaluate the baseline model in a similar fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3256589-b210-49d6-a735-07a48af1eb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_model(\n",
    "        dataset: QuestionAnswerDataset,\n",
    "        model: LLM | RAG,\n",
    "        *,\n",
    "        progress: bool = True,\n",
    ") -> list[Answer]:\n",
    "    if progress:\n",
    "        dataset = tqdm.tqdm(dataset)\n",
    "    answers = [model(sample.query) for sample in dataset]\n",
    "    return answers\n",
    "    \n",
    "def evaluate_model(\n",
    "    dataset: QuestionAnswerDataset,\n",
    "    pipeline: LLM | RAG,\n",
    "    raters: dict[str, Rater],\n",
    "    *,\n",
    "    progress: bool = True,\n",
    ") -> tuple[dict[str, float], list[Answer], list[Sequence[Document]]]:\n",
    "    answers = infer_model(dataset, pipeline, progress=progress)\n",
    "    expected_answers = [sample.answer for sample in dataset]\n",
    "    results = evaluate_answers(answers, expected_answers, raters, progress=progress)\n",
    "    return results, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d43b61ba124d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_results, rag_answers = evaluate_model(partial_dataset, rag, raters)\n",
    "rag_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f26651cdab023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results, model_answers = evaluate_model(partial_dataset, model, raters)\n",
    "model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce10414e-b7f2-4185-8fd7-15f6ad97e8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"Vanilla LLM\": model_results, \"RAG\": rag_results, \"RAG (Generator only)\": generator_results}).plot.bar(figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cac725f-bf3c-4ab5-a18e-a1cfbd5d35fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad7a3af-f5c2-409f-8e64-207992af8288",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5f917e-e111-4923-850d-daea02edbdf6",
   "metadata": {},
   "source": [
    "This work provided a foundational implementation for each model component. For those interested in **further refinement**, significant improvements can be achieved by investigating the following individual components and techniques:\n",
    "\n",
    "* **Document Chunking**: This technique involves splitting source documents into smaller text segments (chunks). The benefit is a **reduction in the context window size** required by the Generator, which improves efficiency. The trade-off, however, is an **increase in the overall corpus size**, which impacts the time and memory complexity of the retrieval pipeline.\n",
    "* **Hybrid & Asymmetrical Embedding Methods**:\n",
    "    * **Hybrid Methods** involve aggregating the outputs of several distinct embedding techniques (e.g., sparse and dense methods) to generate a more robust document embedding.\n",
    "    * **Asymmetrical Methods** use different embedding models specifically optimized for the unique characteristics of the user query (short, conversational) versus the document chunks (long, factual).\n",
    "* **Better Large Language Models (LLMs)**: The pipeline was designed to be **model-agnostic**. You can investigate and experiment with different combinations of LLMs for the various roles (embedding, generation, and evaluation) to determine which models are best suited for each specific retrieval, generation, or evaluation task.\n",
    "* **\"Agentic RAG\"**: Standard RAG is limited because it must always pull information exclusively from the existing document corpus. A powerful extension is to integrate RAG into a broader **Agent** framework. In this approach, the Retrieval Component is treated as one **tool** an agent can choose to call. The agent can then use this tool (or other non-retrieval tools, like web searches or code execution) to gather information, implicitly driving the final generation based on the results of the tool calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b0039e-2f53-4a07-bf4c-094394f3de9f",
   "metadata": {},
   "source": [
    "### Bonus: Agentic RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3911fc8-d7eb-495f-a121-0c40674f9680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hslu.dlm03.common import agent, backend, chat, chat_display, tools, types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2a367c-e5f1-4ce3-817a-59f175a2abdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcp.server import FastMCP\n",
    "\n",
    "SERVER = FastMCP()\n",
    "\n",
    "\n",
    "@SERVER.tool()\n",
    "def retrieve(query: str, k: int | None = None, threshold: float | None = None) -> float:\n",
    "    \"\"\"Retrieves the k most relevant for the given query (that are at least above the given similarity threshold).\"\"\"\n",
    "    documents, similarity = retriever.retrieve(Query(text=query), k)\n",
    "    if threshold:\n",
    "        documents = [document for document, similarity in zip(documents, similarity) if similarity > threshold]\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1748b87b-f56d-492c-8d49-39d5716c968b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "import uvicorn\n",
    "\n",
    "PORT = 5000\n",
    "HOST = \"localhost\"\n",
    "\n",
    "RUN_ARGS = {\n",
    "    \"app\": SERVER.streamable_http_app,\n",
    "    \"port\": PORT,\n",
    "    \"host\": HOST,\n",
    "}\n",
    "\n",
    "MCP_THREAD = threading.Thread(target=uvicorn.run, kwargs=RUN_ARGS)\n",
    "MCP_THREAD.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915db7ee-7616-4d9b-a432-1cf7cebd10c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MCP_SERVER_URL = f\"http://{HOST}:{PORT}/mcp\"\n",
    "TOOL_MANAGER = tools.ToolManager.from_url(MCP_SERVER_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a2617b-5b7a-4a39-80c3-8475fb6586ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "AGENT_BACKEND = backend.LLamaCpp(base_url=MODEL_BASE_URL, ratelimit=MODEL_RPM).get_async_backend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cee7d6-bb37-4dae-8809-b55634255f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_INSTRUCTIONS = \"\"\"You are a helpful assistant tasked with answering user questions.\n",
    "You should use the tools provided to you to ensure the answer is factual by finding the answer in relevant documents.\n",
    "Please give a consice final answer the given user query (do not write full sentences, just provide a direct answer).\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916738f2-074c-4526-90f5-37ddc7c23fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "AGENT = agent.Agent(AGENT_BACKEND, TOOL_MANAGER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dee2613-4ea9-4c74-b9e5-a80233757e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def infer_agent(\n",
    "        dataset: QuestionAnswerDataset,\n",
    "        agent: agent.Agent,\n",
    "        system_instuctions: str,\n",
    "        *,\n",
    "        progress: bool = True,\n",
    ") -> list[Answer]:\n",
    "    if progress:\n",
    "        dataset = tqdm.tqdm(dataset)\n",
    "    answers = []\n",
    "    for sample in dataset:\n",
    "        sample_chat = chat.Chat(messages=[{\"role\": \"system\", \"content\": system_instuctions}, {\"role\": \"user\", \"content\": sample.query.text}])\n",
    "        messages = await agent(sample_chat)\n",
    "        answers.append(Answer(text=messages[-1].content))\n",
    "    return answers\n",
    "    \n",
    "async def evaluate_agent(\n",
    "    dataset: QuestionAnswerDataset,\n",
    "    agent: agent.Agent,\n",
    "    system_instuctions: str,\n",
    "    raters: dict[str, Rater],\n",
    "    *,\n",
    "    progress: bool = True,\n",
    ") -> tuple[dict[str, float], list[Answer], list[Sequence[Document]]]:\n",
    "    answers = await infer_agent(dataset, agent, system_instuctions, progress=progress)\n",
    "    expected_answers = [sample.answer for sample in dataset]\n",
    "    results = evaluate_answers(answers, expected_answers, raters, progress=progress)\n",
    "    return results, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf51d9a-7e62-4aa5-b9e6-efc824b55d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_results, agent_answers = await evaluate_agent(partial_dataset, AGENT, SYSTEM_INSTRUCTIONS, raters)\n",
    "agent_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34b6f5b-6ead-4b5e-ad41-d94e1ba083ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"Vanilla LLM\": model_results, \"RAG\": rag_results, \"RAG (Generator only)\": generator_results, \"Agentic RAG\": agent_results}).plot.bar(figsize=(10, 10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
